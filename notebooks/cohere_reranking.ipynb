{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere Command-R and Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set API Keys\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "langchain_tracing_v2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "from langchain.vectorstores.deeplake import DeepLake\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI # Import OpenAI LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in deeplake_vstore already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "#Initializing CohereEmbeddings and DeepLake\n",
    "embeddings = CohereEmbeddings(\n",
    "    model=\"embed-english-v3.0\",\n",
    "    cohere_api_key= cohere_api_key,\n",
    ")\n",
    "\n",
    "docstore = DeepLake(\n",
    "    dataset_path=\"deeplake_vstore\",\n",
    "    embedding=embeddings,\n",
    "    verbose=False,\n",
    "    num_workers=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 314 embeddings in 1 batches of size 314:: 100%|██████████| 1/1 [00:20<00:00, 20.56s/it]\n"
     ]
    }
   ],
   "source": [
    "#Ingesting Document into Vector Store\n",
    "file = \"/home/martin/Documents/LegalAI-RAG-Contract-Advisor/data/Raptor_Contract.pdf\"\n",
    "loader = PyPDFLoader(file_path=file)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "pages = loader.load()\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "_ = docstore.add_documents(documents=chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in deeplake_vstore already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "#Initializing DeepLake Vector Store and ChatCohere \n",
    "docstore = DeepLake(\n",
    "    dataset_path=\"deeplake_vstore\",\n",
    "    embedding=embeddings,\n",
    "    verbose=False,\n",
    "    read_only=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "llm = ChatCohere(\n",
    "    model=\"command\",\n",
    "    cohere_api_key=cohere_api_key,\n",
    "    temperature=0.7,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Retriever Object\n",
    "retriever = docstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"fetch_k\": 20,\n",
    "        \"k\": 10,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Prompt Template and Initializing RetrievalQA Chain\n",
    "prompt_template = \"\"\"You are an intelligent chatbot that can answer user's \n",
    "queries. You will be provided with Relevant context based on the user's queries. \n",
    "Your task is to analyze user's query and generate response for the query \n",
    "utiliing the context. Make sure to suggest one similar follow-up question based \n",
    "on the context for the user to ask.\n",
    "\n",
    "NEVER generate response to queries for which there is no or irrelevant context.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "chain_type_kwargs = {\"prompt\": PROMPT, \"verbose\": False}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=False,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the agreement, each Seller is individually responsible for any breach of representations and warranties they have made to the Buyer. This means that if a Seller breaches a representation or warranty, they are liable for the consequences of that breach. The extent of this liability would depend on the specific terms outlined in the agreement and the nature of the breach.\n",
      "\n",
      "Follow-up question: How are breaches of representations and warranties typically resolved in such agreements?\n"
     ]
    }
   ],
   "source": [
    "#Generating a Response to a Query\n",
    "query = \"Under what circumstances and to what extent the Sellers are responsible for a breach of representations and warranties?\"\n",
    "response = qa.invoke({\"query\": query})\n",
    "result = response[\"result\"]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Cohere’s Reranker Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Modules\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in deeplake_vstore already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "#Creating the Reranker Object and Compression Retriever\n",
    "cohere_rerank = CohereRerank(\n",
    "    cohere_api_key=\"\",\n",
    "    model=\"rerank-english-v3.0\",\n",
    ")\n",
    "docstore = DeepLake(\n",
    "    dataset_path=\"deeplake_vstore\",\n",
    "    embedding=embeddings,\n",
    "    verbose=False,\n",
    "    read_only=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=cohere_rerank,\n",
    "    base_retriever=docstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\n",
    "            \"fetch_k\": 20,\n",
    "            \"k\": 15,\n",
    "        },\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an intelligent chatbot that can answer user's \n",
    "queries. You will be provided with Relevant context based on the user's queries. \n",
    "Your task is to analyze user's query and generate response for the query \n",
    "utiliing the context. Make sure to suggest one similar follow-up question based \n",
    "on the context for the user to ask.\n",
    "\n",
    "NEVER generate response to queries for which there is no or irrelevant context.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT, \"verbose\": False}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=False,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the agreement, each Seller is individually responsible for their representations and warranties. They are required to indemnify the Sellers' Representative from any losses arising out of serving in that capacity, except for losses caused by gross negligence, bad faith, or willful misconduct. The extent of their responsibility depends on the specific circumstances of the breach and is determined by their Pro Rata Percentage. \n",
      "\n",
      "Follow-up question: How are Pro Rata Percentages determined in the context of the agreement between the Sellers and the Buyers?\n"
     ]
    }
   ],
   "source": [
    "#Generating a Response to a Query\n",
    "query = \"Under what circumstances and to what extent the Sellers are responsible for a breach of representations and warranties?\"\n",
    "response = qa.invoke({\"query\": query})\n",
    "result = response[\"result\"]\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
